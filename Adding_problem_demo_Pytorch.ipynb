{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "import torch\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(N, seq_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_length: Length of the adding problem data\n",
    "        N: # of data in the set\n",
    "    \"\"\"\n",
    "    X_num = torch.rand([N, 1, seq_length])\n",
    "    X_mask = torch.zeros([N, 1, seq_length])\n",
    "    Y = torch.zeros([N, 1])\n",
    "    for i in range(N):\n",
    "        positions = np.random.choice(seq_length, size=2, replace=False)\n",
    "        X_mask[i, 0, positions[0]] = 1\n",
    "        X_mask[i, 0, positions[1]] = 1\n",
    "        Y[i,0] = X_num[i, 0, positions[0]] + X_num[i, 0, positions[1]]\n",
    "    X = torch.cat((X_num, X_mask), dim=1)\n",
    "    return Variable(X), Variable(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = data_generator(50000, seq_length=100)\n",
    "X_test, Y_test = data_generator(1000, seq_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_pad_same(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Conv1d_pad_same, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Conv1d_pad_same(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Conv1d_pad_same(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 2\n",
    "n_classes = 1\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "epochs = 10\n",
    "nhid = 24\n",
    "levels = 5\n",
    "kernel_size = 7\n",
    "dropout = 0.\n",
    "lr = 0.002\n",
    "channel_sizes = [nhid]*levels\n",
    "log_interval = 100\n",
    "\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "X_train = X_train.cuda()\n",
    "Y_train = Y_train.cuda()\n",
    "X_test = X_test.cuda()\n",
    "Y_test = Y_test.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global lr\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    for i in range(0, X_train.size()[0], batch_size):\n",
    "        if i + batch_size > X_train.size()[0]:\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = F.mse_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            processed = min(i+batch_size, X_train.size()[0])\n",
    "            print('Train Epoch: {:2d} [{:6d}/{:6d} ({:.0f}%)]\\tLearning rate: {:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, processed, X_train.size()[0], 100.*processed/X_train.size()[0], lr, cur_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    output = model(X_test)\n",
    "    test_loss = F.mse_loss(output, Y_test)\n",
    "    print('\\nTest set: Average loss: {:.6f}\\n'.format(test_loss.data[0]))\n",
    "    return test_loss.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN(\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(2, 24, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "        (chomp1): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0)\n",
      "        (conv2): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "        (chomp2): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(2, 24, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "          (1): Conv1d_pad_same(\n",
      "          )\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0)\n",
      "          (4): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "          (5): Conv1d_pad_same(\n",
      "          )\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0)\n",
      "        )\n",
      "        (downsample): Conv1d(2, 24, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "        (chomp1): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0)\n",
      "        (conv2): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "        (chomp2): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "          (1): Conv1d_pad_same(\n",
      "          )\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0)\n",
      "          (4): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "          (5): Conv1d_pad_same(\n",
      "          )\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "        (chomp1): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0)\n",
      "        (conv2): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "        (chomp2): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "          (1): Conv1d_pad_same(\n",
      "          )\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0)\n",
      "          (4): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "          (5): Conv1d_pad_same(\n",
      "          )\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
      "        (chomp1): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0)\n",
      "        (conv2): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
      "        (chomp2): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
      "          (1): Conv1d_pad_same(\n",
      "          )\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0)\n",
      "          (4): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
      "          (5): Conv1d_pad_same(\n",
      "          )\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): TemporalBlock(\n",
      "        (conv1): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n",
      "        (chomp1): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0)\n",
      "        (conv2): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n",
      "        (chomp2): Conv1d_pad_same(\n",
      "        )\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n",
      "          (1): Conv1d_pad_same(\n",
      "          )\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0)\n",
      "          (4): Conv1d(24, 24, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n",
      "          (5): Conv1d_pad_same(\n",
      "          )\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=24, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  1 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.241648\n",
      "Train Epoch:  1 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.165669\n",
      "Train Epoch:  1 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.146444\n",
      "\n",
      "Test set: Average loss: 0.019186\n",
      "\n",
      "Train Epoch:  2 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.014118\n",
      "Train Epoch:  2 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.011224\n",
      "Train Epoch:  2 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.007150\n",
      "\n",
      "Test set: Average loss: 0.006770\n",
      "\n",
      "Train Epoch:  3 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.005005\n",
      "Train Epoch:  3 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.004993\n",
      "Train Epoch:  3 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.004100\n",
      "\n",
      "Test set: Average loss: 0.005723\n",
      "\n",
      "Train Epoch:  4 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.003220\n",
      "Train Epoch:  4 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.003243\n",
      "Train Epoch:  4 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.002846\n",
      "\n",
      "Test set: Average loss: 0.001981\n",
      "\n",
      "Train Epoch:  5 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.002302\n",
      "Train Epoch:  5 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.002373\n",
      "Train Epoch:  5 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.002024\n",
      "\n",
      "Test set: Average loss: 0.002204\n",
      "\n",
      "Train Epoch:  6 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.001707\n",
      "Train Epoch:  6 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.001808\n",
      "Train Epoch:  6 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.001634\n",
      "\n",
      "Test set: Average loss: 0.001169\n",
      "\n",
      "Train Epoch:  7 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.001506\n",
      "Train Epoch:  7 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.001516\n",
      "Train Epoch:  7 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.001416\n",
      "\n",
      "Test set: Average loss: 0.000999\n",
      "\n",
      "Train Epoch:  8 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.001314\n",
      "Train Epoch:  8 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.001198\n",
      "Train Epoch:  8 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.001230\n",
      "\n",
      "Test set: Average loss: 0.001330\n",
      "\n",
      "Train Epoch:  9 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.001314\n",
      "Train Epoch:  9 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.001027\n",
      "Train Epoch:  9 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.000988\n",
      "\n",
      "Test set: Average loss: 0.001697\n",
      "\n",
      "Train Epoch: 10 [ 12672/ 50000 (25%)]\tLearning rate: 0.0020\tLoss: 0.000740\n",
      "Train Epoch: 10 [ 25472/ 50000 (51%)]\tLearning rate: 0.0020\tLoss: 0.000735\n",
      "Train Epoch: 10 [ 38272/ 50000 (77%)]\tLearning rate: 0.0020\tLoss: 0.000918\n",
      "\n",
      "Test set: Average loss: 0.000725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(1, epochs+1):\n",
    "    train(ep)\n",
    "    tloss = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(X_test[10:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.616028  , 0.90313846, 0.06202805, 0.86564493, 0.34850854,\n",
       "         0.8607909 , 0.2327509 , 0.45310503, 0.46806705, 0.74760807,\n",
       "         0.21059823, 0.55009705, 0.5269502 , 0.39872706, 0.05495358,\n",
       "         0.7289608 , 0.8472756 , 0.6520019 , 0.10983479, 0.06915188,\n",
       "         0.4299478 , 0.27376783, 0.3172034 , 0.21797365, 0.77016574,\n",
       "         0.4539659 , 0.21675795, 0.13673985, 0.01736778, 0.497369  ,\n",
       "         0.10169131, 0.2664454 , 0.02172685, 0.53324074, 0.98921514,\n",
       "         0.9557614 , 0.38263506, 0.03936386, 0.97934014, 0.334042  ,\n",
       "         0.04893422, 0.7116153 , 0.18837929, 0.21946335, 0.57322216,\n",
       "         0.4681689 , 0.57627976, 0.5808097 , 0.26296347, 0.21397907,\n",
       "         0.00286973, 0.91633177, 0.79443693, 0.3584674 , 0.56790763,\n",
       "         0.655529  , 0.02952695, 0.4447524 , 0.51078296, 0.41392833,\n",
       "         0.3139944 , 0.9475292 , 0.26595765, 0.6437272 , 0.7807376 ,\n",
       "         0.04602867, 0.8947014 , 0.690051  , 0.5586796 , 0.43738782,\n",
       "         0.597533  , 0.46625042, 0.43326443, 0.65169626, 0.49843395,\n",
       "         0.2509551 , 0.8186784 , 0.60685635, 0.0698418 , 0.69283926,\n",
       "         0.7697811 , 0.39085066, 0.29085755, 0.965589  , 0.58209527,\n",
       "         0.10500693, 0.8083296 , 0.79074365, 0.37342316, 0.8220535 ,\n",
       "         0.4040026 , 0.05918437, 0.564043  , 0.7132515 , 0.73717105,\n",
       "         0.48363256, 0.0180552 , 0.44088966, 0.10399634, 0.18114138],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[10:11].data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.020756"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.606856+0.4139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0138\n",
       "[torch.cuda.FloatTensor of size 1x1 (GPU 0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
